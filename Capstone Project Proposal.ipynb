{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unintended Bias in toxicity classification\n",
    "\n",
    "## Udacity Machine Learning Engineer Nanodegree\n",
    "\n",
    "- _Author: Giuseppe Romagnuolo_\n",
    "- _Project from the Kaggle competition: [Jigsaw unintended bias in toxicity classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview)_\n",
    "- _Field: Natural Language Processing_<br/>\n",
    "\n",
    "---------------------\n",
    "\n",
    "## Domain Background\n",
    "\n",
    "Natural Language Processing is a complex field which is hypothesised to be part of AI-complete set of problems, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem making computers as intelligent as people.[$^{[cit.Wikipedia]}$](https://en.wikipedia.org/wiki/AI-complete)\n",
    "\n",
    "With over 90% of data ever generated being produced in the last 2 years[$^{[ref.ScienceDaily]}$](https://www.sciencedaily.com/releases/2013/05/130522085217.htm) and with a great proportion being human generated unstructured text there is an ever increasing need to advance the field of Natural Language Processing.\n",
    "\n",
    "Recent UK Government proposal to have measures to regulate social media companies over harmful content, including \"substantial\" fines and the ability to block services that do not stick to the rules is an example of a regulatory requirement to better manage the content that is being generated by users. [$^{[ref.BBC]}$](https://www.bbc.co.uk/news/technology-47135058)\n",
    "\n",
    "Other initiatives like [Riot Games](https://www.riotgames.com/en)'s work aimed to predict and reform toxic player behaviour during games[$^{[ref.ArsTechnica]}$](https://arstechnica.com/gaming/2013/05/using-science-to-reform-toxic-player-behavior-in-league-of-legends/) is another example of this effort to understand the content being generated by users and moderate toxic content.\n",
    "\n",
    "However, as highlighted by the Kaggle competition [Jigsaw unintended bias in toxicity classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview), existing models suffer from unintended bias where models might predict high likelihood of toxicity for content containing certain words (e.g. \"gay\") even when those comments were not actually toxic (such as \"I am a gay woman\"), leaving  machine only classification models still sub-standard.\n",
    "\n",
    "Having tools that are able to flag up toxic content without suffering from unintended bias is of paramount importance to preserve Internet's fairness and freedom of speech.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "From [Kaggle](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview) competition page:\n",
    "\n",
    "The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.\n",
    "\n",
    "Last year, in the Toxic Comment Classification Challenge, participants built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.\n",
    "\n",
    "Here’s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users.\n",
    "\n",
    "In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Inputs\n",
    "\n",
    "From [Kaggle](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data) dataset description:\n",
    "\n",
    "_Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive._\n",
    "\n",
    "### Background\n",
    "\n",
    "At the end of 2017 the [Civil Comments](https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d) platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n",
    "\n",
    "In the data supplied for this competition, the text of the individual comment is found in the `comment_text` column. Each comment in Train has a toxicity label (`target`), and models should predict the `target` toxicity for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment. For evaluation, test set examples with `target >= 0.5` will be considered to be in the positive class (toxic).\n",
    "\n",
    "The data also has several additional toxicity subtype attributes. Models do not need to predict these attributes for the competition, they are included as an additional avenue for research. Subtype attributes are:\n",
    "\n",
    "-   severe_toxicity\n",
    "-   obscene\n",
    "-   threat\n",
    "-   insult\n",
    "-   identity_attack\n",
    "-   sexual_explicit\n",
    "\n",
    "Additionally, a subset of comments have been labelled with a variety of identity attributes, representing the identities that are *mentioned* in the comment. The columns corresponding to identity attributes are listed below. Only identities with more than 500 examples in the test set (combined public and private) will be included in the evaluation calculation. These identities are shown in bold.\n",
    "\n",
    "-   male\n",
    "-   female\n",
    "-   transgender\n",
    "-   other_gender\n",
    "-   heterosexual\n",
    "-   homosexual_gay_or_lesbian\n",
    "-   bisexual\n",
    "-   other_sexual_orientation\n",
    "-   christian\n",
    "-   jewish\n",
    "-   muslim\n",
    "-   hindu\n",
    "-   buddhist\n",
    "-   atheist\n",
    "-   other_religion\n",
    "-   black\n",
    "-   white\n",
    "-   asian\n",
    "-   latino\n",
    "-   other_race_or_ethnicity\n",
    "-   physical_disability\n",
    "-   intellectual_or_learning_disability\n",
    "-   psychiatric_or_mental_illness\n",
    "-   other_disability\n",
    "\n",
    "Note that the data contains different comments that can have the exact same text. Different comments that have the same text may have been labeled with different targets or subgroups.\n",
    "\n",
    "### Examples\n",
    "\n",
    "\n",
    "Here are a few examples of comments and their associated toxicity and identity labels. Label values range from 0.0 - 1.0 represented the fraction of raters who believed the label fit the comment.\n",
    "\n",
    "Comment: *i'm a white woman in my late 60's and believe me, they are not too crazy about me either!!*\n",
    "\n",
    "-   Toxicity Labels: All 0.0\n",
    "-   Identity Mention Labels: female: 1.0, white: 1.0 (all others 0.0)\n",
    "\n",
    "Comment: *Why would you assume that the nurses in this story were women?*\n",
    "\n",
    "-   Toxicity Labels: All 0.0\n",
    "-   Identity Mention Labels: female: 0.8 (all others 0.0)\n",
    "\n",
    "Comment: *Continue to stand strong LGBT community. Yes, indeed, you'll overcome and you have.*\n",
    "\n",
    "-   Toxicity Labels: All 0.0\n",
    "-   Identity Mention Labels: homosexual_gay_or_lesbian: 0.8, bisexual: 0.6, transgender: 0.3 (all others 0.0)\n",
    "\n",
    "In addition to the labels described above, the dataset also provides metadata from Jigsaw's annotation: `toxicity_annotator_count` and `identity_annotator_count`, and metadata from Civil Comments: `created_date`, `publication_id`, `parent_id`, `article_id`, `rating`, `funny`, `wow`, `sad`, `likes`, `disagree`. Civil Comments' label `rating` is the civility rating Civil Comments users gave the comment.\n",
    "\n",
    "### Labelling Schema\n",
    "\n",
    "\n",
    "To obtain the toxicity labels, each comment was shown to up to 10 annotators. Annotators were asked to: \"Rate the toxicity of this comment\"\n",
    "\n",
    "-   Very Toxic (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n",
    "-   Toxic (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n",
    "-   Hard to Say\n",
    "-   Not Toxic\n",
    "\n",
    "These ratings were then aggregated with the `target` value representing the fraction of annotations that annotations fell within the former two categories.\n",
    "\n",
    "To collect the identity labels, annotators were asked to indicate all identities that were mentioned in the comment. An example question that was asked as part of this annotation effort was: \"What genders are mentioned in the comment?\"\n",
    "\n",
    "-   Male\n",
    "-   Female\n",
    "-   Transgender\n",
    "-   Other gender\n",
    "-   No gender mentioned\n",
    "\n",
    "Again, these were aggregated into fractional values representing the fraction of raters who said the identity was mentioned in the comment.\n",
    "\n",
    "The distributions of labels and subgroup between Train and Test can be assumed to be similar, but not exact.\n",
    "\n",
    "### File descriptions\n",
    "\n",
    "\n",
    "-   train.csv - the training set, which includes subgroups\n",
    "-   test.csv - the test set, which does not include subgroups\n",
    "-   sample_submission.csv - a sample submission file in the correct format\n",
    "\n",
    "### Usage\n",
    "\n",
    "This dataset is released under [CC0](https://creativecommons.org/share-your-work/public-domain/cc0/), as is the [underlying comment text](https://figshare.com/articles/data_json/7376747)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [earlier competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) from Jigsaw saw participants classify toxic/non-toxic comments, however, the models that were built back then fell short when it came to eliminate unintended bias, e.g. they gave a  high likelihood of toxicity for comments containing certain identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happened because training data was pulled from available sources where certain identities are overwhelmingly referred to in offensive ways.[[$^{cit.Kaggle}$]](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)\n",
    "\n",
    "Key to this competition is to minimise this bias and evaluate a strategy able to classify with the same high level of accuracy samples belonging to [different identities](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n",
    "\n",
    "One solution could be the use of an **ensamble** of classifiers where each classifier is optimised on the classification of a particular identity. The likelihood of a comment being toxic would then be based on a weighted average of each single classifier where the weight given to each classifier is adjusted based on the identity of the message.\n",
    "\n",
    "Not all data in the traning set will have been tagged with identities and therefore, in order to adjust the weight of the different classifier based on the comment's identity, there will be the need to predict the likely identity a of a comment.\n",
    "\n",
    "This might require an initial classifier to first predict the possible identity of each comment, the prediction will be used as a feature for the ensamble of classifiers and used to calculate the weight for their score based on the predicted identity of the comment.\n",
    "\n",
    "There will be different models suitable for classification and during the _Model Building_ phase (discussed in Project Design section) I will have to evaluate the ML algorithm to use. Deep Neural Networks like CNN and LSTM are some that I'm considering to evaluate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model\n",
    "\n",
    "As any other Kaggle competition, submissions will be benchmarked against the test set held by Jigsaw producing a score as per the _Evaluation metrics_ defined below.\n",
    "\n",
    "Submissions to this competition must be made through Kernels, also the following conditions must be met:\n",
    "\n",
    "-   CPU Kernel <= 9 hours run-time\n",
    "-   GPU Kernel <= 2 hours run-time\n",
    "-   No internet access enabled\n",
    "-   External data, freely & publicly available, is allowed, including pre-trained models\n",
    "-   No custom packages enabled in kernels\n",
    "-   Submission file must be named \"submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "This competition uses a newly developed metric that combines several submetrics to balance overall performance with various aspects of unintended bias.\n",
    "\n",
    "Please refer to [evaluation section](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation) of the competition and the provided [benchmark kernel](https://www.kaggle.com/dborkan/benchmark-kernel) with code to calculate the competition evaluation metrics.\n",
    "\n",
    "Here are defined the submetrics:\n",
    "\n",
    "### Overall AUC\n",
    "This is the ROC-AUC for the full evaluation set.\n",
    "\n",
    "\n",
    "### Bias AUCs\n",
    "\n",
    "To measure unintended bias, we again calculate the ROC-AUC, this time on three specific subsets of the test set for each identity, each capturing a different aspect of unintended bias. You can learn more about these metrics in Conversation AI's recent paper *[Nuanced Metrics for Measuring Unintended Bias with Real Data in Text Classification](https://arxiv.org/abs/1903.04561)*.\n",
    "\n",
    "**Subgroup AUC:** Here, we restrict the data set to only the examples that mention the specific identity subgroup. *A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity*.\n",
    "\n",
    "**BPSN (Background Positive, Subgroup Negative) AUC:** Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. *A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not*, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.\n",
    "\n",
    "**BNSP (Background Negative, Subgroup Positive) AUC:** Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. *A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not*, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.\n",
    "\n",
    "### Generalized Mean of Bias AUCs\n",
    "\n",
    "To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
    "\n",
    "\n",
    "$M_p(m_s)=( \\frac{1}{N} \\sum_{s=1}^{N}m_{s}^{p} )^{\\frac{1}{p}}$\n",
    "\n",
    "where:\n",
    "\n",
    "MpMp = the ppth power-mean function\\\n",
    "msms = the bias metric mm calulated for subgroup ss\\\n",
    "NN = number of identity subgroups\n",
    "\n",
    "For this competition, we use a pp value of -5 to encourage competitors to improve the model for the identity subgroups with the lowest model performance.\n",
    "\n",
    "### Final Metric\n",
    "\n",
    "We combine the overall AUC with the generalized mean of the Bias AUCs to calculate the final model score:\n",
    "\n",
    "$score=w_0AUC_{overall}+\\sum_{a=1}^{A}w_aM_p(m_{s,a})$\n",
    "\n",
    "where:\n",
    "\n",
    "- $A$ = number of submetrics (3)\n",
    "- $ms_{s,a}$ = bias metric for identity subgroup ss using submetric $a$\n",
    "$w_a$ = a weighting for the relative importance of each submetric; all four $w$ values set to 0.25\n",
    "\n",
    "While the leaderboard will be determined by this single number, we highly recommend looking at the individual submetric results, [as shown in this kernel](https://www.kaggle.com/dborkan/benchmark-kernel), to guide you as you develop your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Project Design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the high level steps I'm planning to take to build a toxic content classification algorithm:\n",
    "\n",
    "1. Exploring data and collecting key metrics\n",
    "2. Definition of evaluation metrics\n",
    "3. Transformation\n",
    "4. Model selection and building\n",
    "5. Training, evaluation and fine tuning the model\n",
    "6. Kaggle submission\n",
    "\n",
    "### 1. Exploring data and collecting key metrics\n",
    "\n",
    "First and foremost I will explore the dataset, during this phase I will gather various statistics like:\n",
    "\n",
    "- Number of samples in the training set\n",
    "- Distribution of toxic/non-toxic in the training set\n",
    "- Distribution of toxic/non-toxic in the training set for each identity\n",
    "- Number of samples with no identity set\n",
    "- Distribution of toxic/non-toxic in the training set for each sub-group\n",
    "- Number of words per sample\n",
    "- Frequency distribution of words\n",
    "- Distribution of sample length\n",
    "- Most frequent words\n",
    "\n",
    "### 2. Definition of evaluation metrics\n",
    "\n",
    "It is important to have defined a method of evaluation from the very beginning and the Kaggle competition provides  what are the evaluation metrics used.\n",
    "\n",
    "However I will want to being able to calculate the score offline without the need to submit the code so I'll want to setup code that can calculate the evaluation metrics offline.\n",
    "\n",
    "### 3. Transformation\n",
    "\n",
    "The dataset will need to be transformed before it can be submitted to a model, this requires the removal of stopwords, identification of punctuation (with consideration of punctuation when used to obfuscate offensive words), lemmitisation and tokenisation. Lastly I will transform words into embeddings evaluating libraries like **Word2Vec**, **GloVe** and **FastText**.\n",
    "\n",
    "One thing to note is that Word2Vec and GloVe only learns vectors for completed words found in the training corpus which means that words that are misspelled or that are camuflaged using special characters like $#!* won't be assigned a vector. \n",
    "\n",
    "On the other hand FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word apple is app, ppl, and ple. Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words. [$^{[cit.Medium]}$](https://medium.com/@jatinmandav3/opinion-mining-sometimes-known-as-sentiment-analysis-or-emotion-ai-refers-to-the-use-of-natural-874f369194c0)\n",
    "\n",
    "\n",
    "### 4. Model selection and building\n",
    "\n",
    "An [earlier competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) from Jigsaw saw participants classify toxic/non-toxic comments, however, the models that were built back then fell short when it came to eliminate unintended bias, e.g. they gave a  high likelihood of toxicity for comments containing certain identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happened because training data was pulled from available sources where certain identities are overwhelmingly referred to in offensive ways.[[$^{cit.Kaggle}$]](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)\n",
    "\n",
    "Key to this competition is to minimise this bias and evaluate a strategy able to classify with the same high level of accuracy samples belonging to [different identities](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n",
    "\n",
    "One solution could be the use of an **ensamble** of classifiers where each classifier is optimised on the classification of a particular identity. The likelihood of a comment being toxic would then be based on a weighted average of each single classifier where the weight given to each classifier is adjusted based on the identity of the message.\n",
    "\n",
    "Not all data in the traning set will have been tagged with identities and therefore, in order to adjust the weight of the different classifier based on the comment's identity, there will be the need to predict the likely identity a of a comment.\n",
    "\n",
    "This might require an initial classifier to first predict the possible identity of each comment, the prediction will be used as a feature for the ensamble of classifiers and used to calculate the weight for their score based on the predicted identity of the comment.\n",
    "\n",
    "There will be different models suitable for classification and during this phase I will have to evaluate the ML algorithm to use. Deep Neural Networks like CNN and LSTM are some that I'm considering to evaluate. \n",
    "\n",
    "**Convolutional neural network** are suitable when data has positional information.\n",
    "\n",
    "**Long Short Term Memory neural network**, like RNNs are designed to use sequential data, when the current step has some kind of relation with the previous steps and they are designed to remember things in the long term. [$^{[cit.Quora]}$](https://www.quora.com/Where-does-each-type-of-neural-network-RNN-CNN-LSTM-etc-excel)\n",
    "\n",
    "While sentiment analysis might not need information of position of words, I believe considering the position of words will improve the accuracy across all the [different identities](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data) and will help with unintended bias (e.g. \"is an offensive word being use to quote someone else's comment\" or \"is the tone demeaning without the use of any particular offensive word\" etc.)\n",
    "\n",
    "A further consideration is the Kaggle kernel limits, it imposes a CPU Kernel <= 9 hours run-time and a GPU Kernel <= 2 hours run-time. This limitation will be a guiding factor when deciding what algorithm to choose.\n",
    "\n",
    "### 5. Training, evaluation and fine tuning the model\n",
    "\n",
    "Having defined a suitable model, probably the hardest part will be evaluation and fine tuning of the model.\n",
    "Techniques like Grid-search can help choose more suitable hyperparameters however this process is painstakingly long.\n",
    "\n",
    "### 6. Kaggle submission\n",
    "\n",
    "It might become tempting to submit to Kaggle frequently to evaluate the performance against Jigsaw's scoring and optimise the model against this score.\n",
    "\n",
    "There is various evidence that this is not a winning strategy, in fact there is a chance of slowly overfitting the model on this test set, it might become a good predictor of the test set but it will not be able to generalise. [$^{[ref.Kaggle]}$](http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/)\n",
    "\n",
    "As such, I shall refrain from making too frequent submission or becoming fixated on the Kaggle leaderboard score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [Sequence classification with human attention](https://aclweb.org/anthology/K18-1030)\n",
    "- [Language Learning with BERT - TensorFlow and Deep Learning Singapore](https://youtu.be/0EtD5ybnh_s)\n",
    "- [Abusive Language Detection with Graph Convolutional Networks](https://research.fb.com/wp-content/uploads/2019/04/Abusive-Language-Detection-with-Graph-Convolutional-Networks.pdf?)\n",
    "- [Bag of Tricks for Efficient Text Classification](https://research.fb.com/wp-content/uploads/2016/07/eacl2017.pdf?)\n",
    "- [The Dangers of Overfitting or How to Drop 50 spots in 1 minute](http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/)\n",
    "- [Google SentencePiece](https://github.com/google/sentencepiece)\n",
    "- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
